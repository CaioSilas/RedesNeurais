{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16K2V1XjapL0WY-pldbkGaXG2ixTcL2pd","timestamp":1701088253973},{"file_id":"1ZeFf5GAr6hhJ0GqXRU_UeqDo-Ysufv_Q","timestamp":1699891331003}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lab 6 - BCC406\n","\n","## REDES NEURAIS E APRENDIZAGEM EM PROFUNDIDADE\n","\n","## Convolução e CNN\n","\n","### Prof. Eduardo e Prof. Pedro\n","\n","Objetivos:\n","\n","- Aplicação de filtros em imagens por meio de convolução\n","- Entendimento do uso de stride, padding e pooling\n","- Modelagem de uma rede de convolução para o problema de rec. de face da AT&T\n","- Uso do VGG pr-e-treinado como um extrator de caracterĩsticas\n","- Uso do MobileNet pré-treinado para classificação de faces : transferência de aprenzagem\n","- Notebook baseado em tensorflow e Keras.\n","\n","Data da entrega : 21/11\n","\n","- Complete o código (marcado com ToDo) e quando requisitado, escreva textos diretamente nos notebooks. Onde tiver *None*, substitua pelo seu código.\n","- Execute todo notebook e salve tudo em um PDF **nomeado** como \"NomeSobrenome-LabX.pdf\"\n","- Envie o PDF via google [FORM](https://forms.gle/X282YLArCvEBHGns9)\n"],"metadata":{"id":"GtOtsitDzLuh"}},{"cell_type":"markdown","metadata":{"id":"h6I8kas8Ib9l"},"source":["# 1. Aplicando filtros e entendendo padding, stride e pooling (20pt)"]},{"cell_type":"markdown","source":["## 1.1. Importando pacotes e montando o drive"],"metadata":{"id":"ioIjMIjGtcXJ"}},{"cell_type":"code","metadata":{"id":"aiXCL7SaKjms"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPool2D, AvgPool2D\n","from tensorflow.keras import datasets, layers, models\n","import os\n","import skimage\n","from skimage import io\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7LwHkqHbIGqX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644760238736,"user_tz":180,"elapsed":18144,"user":{"displayName":"JEAN CHRISTOPHER DE OLIVEIRA PINHEIRO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7prlgKyrdZln-DoJkTXInLQlsAi3RVTwm1r27=s64","userId":"05168836363073011217"}},"outputId":"7f171cc2-c1e4-4c1d-e9ce-471cda119805"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from skimage.io import imread\n","from skimage.transform import resize"],"metadata":{"id":"gh_FZVJrtwjJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2. Carregando uma imagem"],"metadata":{"id":"Xxof7XPats1Z"}},{"cell_type":"markdown","source":["Carregue um imagem do disco, para usar como exemplo."],"metadata":{"id":"750747gMta1B"}},{"cell_type":"code","metadata":{"id":"0fFkIda6Ih7b"},"source":["# carrega imagem de exemplo\n","sample_image = imread(\"/content/drive/My Drive/Lenna.png\")\n","sample_image= sample_image.astype(float)\n","\n","size = sample_image.shape\n","print(\"sample image shape: \", sample_image.shape)\n","\n","plt.imshow(sample_image.astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzFhzZjQJMEM"},"source":["# veja o shape da imagem\n","sample_image.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_bOePQnIlEG"},"source":["## 1.3. Criando e aplicando um filtro com convolução"]},{"cell_type":"markdown","source":["Utilize o tf/Keras para aplicar o filtro. Observe que nesta etapa não há necessidade de treinamento algum. O código abaixo cria 3 filtros de tamanho 5x5, e adiciona padding de forma a manter a imagem de saída (filtrada) do mesmo tamanho da imagem de entrada (padding =\"same\")."],"metadata":{"id":"TzDvuzMmt9p6"}},{"cell_type":"code","metadata":{"id":"a9YNRZyiKv_b"},"source":["#cria um objeto sequencial com apenas uma camada de convolução do tipo tf.keras.layers.Conv2D\n","conv = Sequential([\n","    Conv2D(filters=3, kernel_size=(5, 5), padding=\"same\",\n","           input_shape=(None, None, 3))\n","])\n","conv.output_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHcUBhaaLN1S"},"source":["conv.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Tw7QxGJJczf"},"source":["# com TF/kertas, as convoluções esperam vetores no formato : (batch_size, dim1, dim2, dim3). Ou seja, a primeria posição é o tamanho do lote.\n","# Uma imagem isolada é considerada um lote de tamanho 1, portanto, deve-se expandir mais uma dimenão do tensor.\n","img_in = np.expand_dims(sample_image, 0)\n","img_in.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQQZQXb4J6St"},"source":["Agora, pode-se aplicar a convolução. Aplique a convolução na imagem de exemplo (expandida) e verifique o tamanho da imagem resultante (img_out). Use a função predict do objeto conv para aplicar a convolução."]},{"cell_type":"code","metadata":{"id":"5a3CfiytJ-CF"},"source":["img_out = conv(img_in)\n","img_out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rB4nKTebLFCH"},"source":["Plote as imagens lado a lado e observe o resultado. O parâmetro \"same\" no padding aplica um padding automático no sentido de garantir que a saída tenha o mesmo tamanho da entrada.\n","Lembre-se que o padding adiciona zeros nas bordas da imagem, antes da aplicação da convolução."]},{"cell_type":"code","metadata":{"id":"bkYAP2LxLHo-"},"source":["fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax1.imshow(img_out[0].numpy().astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVtXvb-LLSi_"},"source":["Repita o mesmo procedimento, trocando padding de 'same' para 'valid', usando apenas um filtro."]},{"cell_type":"code","metadata":{"id":"nFG2X1O3La2_"},"source":["conv2 = Sequential([\n","    Conv2D(filters=1, kernel_size=(5, 5), padding=\"valid\",\n","           input_shape=(None, None, 3))\n","])\n","conv2.output_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FycvJ_HbMOt1"},"source":["conv2.summary() # 1 filtro 5x5x3 ... a profundidade do filtro é de acordo com a entrada. 5x5x3 = 75; Não esqueça do bias!\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcCK91x6Li7s"},"source":["img_out = conv2(img_in)\n","img_out[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yS2S43lzL1xF"},"source":["Plote as duas imagens lado a lado"]},{"cell_type":"code","metadata":{"id":"ZbI6okrYLyop"},"source":["#  Como tivemos que expandir a primeira dimensao para aplicar a convolução, podemos remover a dimensão unitária para plotar a imagem, usando a função squeeze()\n","i = img_out[0].numpy().squeeze()\n","i.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKlpI-vKL4Kg"},"source":["fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","i = img_out[0].numpy().squeeze()\n","ax1.imshow(i.astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HcPKjTQXMWbj"},"source":["## 1.4. Inicializando os filtros na mão\n"]},{"cell_type":"markdown","source":["A função abaixo inicializa um array de dimensões 5,5,3,3 com todas as posições zero, exceto as posições 5,5,0,0 , 5,5,1,1 e 5,5,2,2 que recebem o valor 1/25."],"metadata":{"id":"AvnWD6KJuQzC"}},{"cell_type":"code","metadata":{"id":"orRVhpK8MlZs"},"source":["def my_filter(shape=(5, 5, 3, 3), dtype=None):\n","    array = np.zeros(shape=shape, dtype=np.float32)\n","    array[:, :, 0, 0] = 1 / 25\n","    array[:, :, 1, 1] = 1 / 25\n","    array[:, :, 2, 2] = 1 / 25\n","    return array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iO2WM7_BMoG_"},"source":["# transposição apenas para ajudar na visualização\n","np.transpose(my_filter(), (2, 3, 0, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jxSfVL_M1ap"},"source":["# a função definida acima é usada para carregar valores nos filtros.\n","# use a função my_filter() para pre-inicializar os filtros do objeto conv3.\n","#\n","conv3 = Sequential([\n","     Conv2D(filters=3, kernel_size=(5, 5), padding=\"same\",\n","           input_shape=(None, None, 3), kernel_initializer=my_filter)\n","])\n","conv3.output_shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PQbsrU9nNVZZ"},"source":["## 1.5. Plote e observe o que aconte com a imagem (1pt)\n","\n"]},{"cell_type":"markdown","source":["Foi observada uma redução da nitidez na imagem."],"metadata":{"id":"Jnypi8EjugpR"}},{"cell_type":"code","metadata":{"id":"iK9IyagzNYF0"},"source":["# observe o que aconte com a imagem\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(img_in[0].astype('uint8'))\n","ax1.imshow(conv3.predict(img_in)[0].astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Responda\n","\n","**ToDo** : Descreva suas observações sobre a imagem anterior."],"metadata":{"id":"60eGKZowshsS"}},{"cell_type":"markdown","source":["## 1.6. Filtros de borda (5pt)"],"metadata":{"id":"MAapLCWnu0t5"}},{"cell_type":"markdown","metadata":{"id":"tyeZPzgENmjJ"},"source":["**ToDo** : Crie uma nova função para gerar um filtro de borda nos 3 canais da imagem de entrada. O filtro deve ser 3x3 e ter o formato [[0 0.2 0] [0 -0.2 0] [0 0 0]] (2pt)"]},{"cell_type":"code","metadata":{"id":"uzYdQ0zwN4-f"},"source":["def my_new_filter(shape=(1, 3, 3, 3), dtype=None):\n","    None\n","    return None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zvxcaAm0OEBW"},"source":["Inicialize o objeto conv4 com seu novo filtro e aplique na imagem de entrada"]},{"cell_type":"code","metadata":{"id":"E2hbw4b8ODZS"},"source":["conv4 = Sequential(\n","    None\n","])\n","conv4.output_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fio-DLbTOQxU"},"source":["# Plote as duas iamgens lado a lado (filtrada e não filtrada)\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(img_in[0].astype('uint8'))\n","ax1.imshow(conv.predict(img_in)[0].astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWwPoQoaQVij"},"source":["## 1.7. Pooling (14pt)\n","\n"]},{"cell_type":"markdown","source":["Aplique um max-pooling na imagem, com uma janela de 2x2. Faça com stride de 2 e observe o resultado na imagem de saída."],"metadata":{"id":"AH4AZV5CvVUp"}},{"cell_type":"code","metadata":{"id":"4xPxbeF4Qw6H"},"source":["# cria um objeto Sequencial de nome max_pool, apenas contendo uma camada de tf.keras.layer.MaxPool2D.\n","# lembre-se de colucar o parametro input-shape como input_shape=(None, None, 3)\n","\n","max_pool = Sequential( # ToDo ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjewDi0CRHQp"},"source":["img_in = np.expand_dims(sample_image, 0) # expande a imagem\n","img_out = max_pool.predict(img_in) # aplica o pooling"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9QJ2PdVR2xJ"},"source":["# plota as imagens lado a lado\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(img_in[0].astype('uint8'))\n","ax1.imshow(img_out[0].astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G_wEmcEtQsiV"},"source":["Aumente o stride para 4, repita o processo e observe o resultado na imagem de saída.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"z1h2QRKnSDmB"},"source":["# cria um objeto Sequencial de nome max_pool, apenas contendo uma camada de tf.keras.layer.MaxPool2D.\n","# lembre-se de colucar o parametro input-shape como input_shape=(None, None, 3)\n","# Coloque o parametro stride para 4\n","\n","max_pool2 = Sequential( # ToDo ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlUE0S0SSM32"},"source":["img_in = np.expand_dims(sample_image, 0) # expande a imagem\n","img_out = max_pool2.predict(img_in) # aplica o pooling"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8sBzHO0SfKW"},"source":["# plota as imagens lado a lado\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(img_in[0].astype('uint8'))\n","ax1.imshow(img_out[0].astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8JmfjDWCSW9Z"},"source":["Aumente o stride para 8, repita o processo e observe o resultado na imagem de saída. A"]},{"cell_type":"code","metadata":{"id":"bYhwM9NASag7"},"source":["# cria um objeto Sequencial de nome max_pool, apenas contendo uma camada de tf.keras.layer.MaxPool2D.\n","# lembre-se de colucar o parametro input-shape como input_shape=(None, None, 3)\n","# Coloque o parametro stride para 4\n","\n","max_pool3 = Sequential( # ToDo ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MQ9af77ScoP"},"source":["img_in = np.expand_dims(sample_image, 0) # expande a imagem\n","img_out = max_pool3.predict(img_in) # aplica o pooling"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLfuHl3nSgqd"},"source":["# plota as imagens lado a lado\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(img_in[0].astype('uint8'))\n","ax1.imshow(img_out[0].astype('uint8'));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EC3cQ8nSmUO"},"source":["### Responda\n","\n","**ToDo** - Descreva o que aconteceu com o aumento do stride."]},{"cell_type":"markdown","metadata":{"id":"p7_CY-tWTECX"},"source":["# 2. Reconhecimento de Faces usando uma rede de convolução (20pt)\n","\n","\n"]},{"cell_type":"markdown","source":["O objetivo desta etapa é classificar faces na base ORL (AT&T) Database (40 individuos x 10 imagens, de resolução 92x112 pixels e 256 níveis de cinza).\n","\n","Baixe as imagens no site http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html ou da pasta dataset do Drive."],"metadata":{"id":"RIcj3otbv68h"}},{"cell_type":"markdown","source":["## 2.1. Preparando os dados (5pt)"],"metadata":{"id":"2V1xdKA3wOl4"}},{"cell_type":"code","metadata":{"id":"WZQn5ybsTaCC"},"source":["# carregue as imagens\n","\n","# inicializa matrizes X e y\n","X = np.empty([400, 112, 92]) # 40 classe com 10 imgs cada, 10304 = 112x92\n","y = np.empty([400, 1])\n","\n","# percorre todos os diretorios da base att e carrega as imagens\n","imgs_path = \"<my path>/datasets/att_faces\"\n","i=0\n","class_id = 0\n","for f in os.listdir(imgs_path):\n","    #print(f)\n","    if f.startswith(\"s\"):\n","        class_id = class_id + 1\n","        for img_path in os.listdir(os.path.join(imgs_path,f)):\n","            if img_path.endswith(\".pgm\"):\n","                #print(img_path)\n","                X[i, :, :] = io.imread(os.path.join(imgs_path,f,img_path))\n","                y[i, :] = class_id\n","                i = i + 1\n","\n","\n","print(\"dimensões da matriz X = \" , X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJ2aX8xoTm5i"},"source":["# Divida os dados em treino e teste (70%-30%) com a função train_test_split\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split( #Todo : complete"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKryDUQBVYnh"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVtK6BmVVai2"},"source":["X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2. Implementando a rede (15pt)"],"metadata":{"id":"Q_9_N7bIwXBB"}},{"cell_type":"markdown","metadata":{"id":"AvVOLXRnT7Yp"},"source":["Implemente uma rede de convolução simples, contendo 3 camadas de convolução seguidas de camadas max-pooling. Duas camadas densas (totalemtne conectadas) no final e por fim uma camada com ativação softmax para a classificação. Escolha filtros de tamanhos variados : (3,3) ou (5,5). Para cada camada, crie de 32 a 96 filtros.\n","Na camada densa, use de 64 a 200 neurônios.\n","\n","Use o comando model.summary() para conferir a arquitetura."]},{"cell_type":"code","metadata":{"id":"AuyzOL8_Udir"},"source":["# Implementa uma rede de convolução simples, chamada model\n","\n","input_size = (X.shape[1], X.shape[2],1)\n","n_classes = 40\n","\n","model = models.Sequential()\n","\n","model.add(layers.InputLayer( # ToDo ...\n","model.add(layers.Conv2D( # Todo ...\n","model.add(layers.MaxPooling2D( # Todo ..\n","\n","# ToDo : adicionar as outras camadas\n","\n","model.add(layers.Flatten()) # não esqueça da camada flatten ..\n","\n","model.add(layers.Dense( # Todo ..\n","model.add(layers.Dense( # Todo: softmax\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_UxtfYFxVF7F"},"source":["Seu modelo deve ter uma saída aproximadamente como abaixo:\n","\n","\n","\n","```\n","Model: \"sequential_36\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_60 (Conv2D)           (None, 110, 90, 32)       320       \n","_________________________________________________________________\n","max_pooling2d_18 (MaxPooling (None, 55, 45, 32)        0         \n","_________________________________________________________________\n","conv2d_61 (Conv2D)           (None, 53, 43, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_19 (MaxPooling (None, 26, 21, 64)        0         \n","_________________________________________________________________\n","conv2d_62 (Conv2D)           (None, 24, 19, 64)        36928     \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 29184)             0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 64)                1867840   \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 40)                2600      \n","=================================================================\n","Total params: 1,926,184\n","Trainable params: 1,926,184\n","Non-trainable params: 0\n","_________________________________________________________________\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"A-1bgTXLVjxo"},"source":["# repare bem o shape de x_train. A priumeira dimensão é o tamanho do lote, a segunda e terceira são referentes ao taamnho das imagens.\n","# repare que as imagens desta base tem apenas uma banda (escala de cinza)\n","X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfF5fxITV1VK"},"source":["# Como o tensor acima não contempla o tamanho de canais (no caso , igual a 1), deve-se expandir a última dimensão para deixar a entrada compatĩvel com o que é esperado pelo modelo do tf.keras\n","X_train_new = np.expand_dims( # Todo ..\n","X_test_new = np.expand_dims( # Todo ..\n","\n","X_train_new.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwWcz94_Wg_Y"},"source":["# o vetor de rótulos não precisa ter duas diemnões.\n","y_train_new = y_train.squeeze()\n","y_test_new = y_test.squeeze()\n","\n","# e deve ficar na faixa entre 0 e 39\n","y_train_new = y_train_new - 1;\n","y_test_new = y_test_new - 1;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gAGaOloWqYe"},"source":["Compile o modelo usando o método de otimização=adam e função de custo (loss) = sparse_categorical_crossentropy."]},{"cell_type":"code","metadata":{"id":"INfxKh-dW1kb"},"source":["model.compile( # Todo ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7kpuR9ASW3yY"},"source":["Treine o modelo por 30 épocas com batch_size = 100."]},{"cell_type":"code","metadata":{"id":"QKBagOnQW8UY"},"source":["history = model.fit( # Todo ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4eFFq1UXCgW"},"source":["O retorno da função fit() é um objeto para armazenar o histõrico do treino."]},{"cell_type":"code","metadata":{"id":"jO1qNLbTXLJ5"},"source":["history.history.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvC_mcoqXOms"},"source":["Plote a acurácia e o custo (loss) do treino e da validação."]},{"cell_type":"code","metadata":{"id":"v3Xf6Q4mXRbT"},"source":["plt.plot(history.history['acc'], label='accuracy')\n","plt.plot(history.history['val_acc'], label = 'val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1.1])\n","plt.legend(loc='lower right')\n","\n","plt.plot(history.history['loss'], label='loss')\n","plt.plot(history.history['val_loss'], label = 'val_loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1.1])\n","plt.legend(loc='lower right')\n","\n","test_loss, test_acc = model.evaluate(X_test_new,  y_test_new, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqZ-m0sgXU9D"},"source":["print(test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THZPjbihXYpq"},"source":["# 3. Usando um modelo Pré-treinado : VGG (10pt)"]},{"cell_type":"markdown","metadata":{"id":"fLwTl8v6Xo-T"},"source":["Carregando os dados da base AT&T para o VGG. Como a base está em escala de cinza e a entrada do modelo VGG espera uma imagem colorida (RGB), vamos repetir a mesma imagem em cada uma das bandas."]},{"cell_type":"markdown","source":["## 3.1. Preparando os dados (2pt)\n"],"metadata":{"id":"7I2aT3rcwpYI"}},{"cell_type":"code","metadata":{"id":"RnpSaR3sXs6z"},"source":["# inicializa matrizes X e y\n","X = np.empty([400, 112, 92, 3]) # 40 classe com 10 imgs cada, 10304 = 112x92\n","y = np.empty([400, 1])\n","\n","# percorre todos os diretorios da base att e carrega as imagens\n","imgs_path = \"<my paths>/datasets/att_faces\"\n","i=0\n","class_id = 0\n","for f in os.listdir(imgs_path):\n","    #print(f)\n","    if f.startswith(\"s\"):\n","        class_id = class_id + 1\n","        for img_path in os.listdir(os.path.join(imgs_path,f)):\n","            if img_path.endswith(\".pgm\"):\n","                #print(img_path)\n","                # copia msg imagem para os 3 canais\n","                X[i, :, :,0] = io.imread(os.path.join(imgs_path,f,img_path))\n","                X[i, :, :,1] = io.imread(os.path.join(imgs_path,f,img_path))\n","                X[i, :, :,2] = io.imread(os.path.join(imgs_path,f,img_path))\n","                y[i, :] = class_id-1\n","                i = i + 1\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-7f5k_KYD1K"},"source":["# divida em 70% treino e 30% teste\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split( # Todo ...\n","X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdwp1AqgYNn5"},"source":["## 3.2. Carrando o VGG direto da biblioteca do tensorflow (2pt)"]},{"cell_type":"code","metadata":{"id":"iXVTuPpHYQ_6"},"source":["# https://www.tensorflow.org/guide/keras/functional?hl=pt_br\n","\n","from tensorflow.keras.applications import VGG19\n","vgg19 = VGG19()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMiaPJ6wYUIz"},"source":["vgg19.summary() # repare a quantidade de parãmetros!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDY6510wYb-z"},"source":["Vamos descartar as duas últimas camadas do VGG"]},{"cell_type":"code","metadata":{"id":"AAaisGc9Yfnu"},"source":["# https://www.tensorflow.org/guide/keras/functional?hl=pt_br\n","from tensorflow.keras.models import Model\n","\n","vgg_face_descriptor = Model(inputs=vgg19.layers[0].input, outputs=vgg19.layers[-2].output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"at9gD07JYg0P"},"source":["### Responda\n","\n","**ToDo** - Por que descartamos as duas últimas camadas do VGG?"]},{"cell_type":"markdown","metadata":{"id":"_wJwUqOIYrsF"},"source":["## 3.3 Medindo Similaridade"]},{"cell_type":"markdown","source":["### As funções abaixo servem para medir similaridade entre duas imagens, passando-se um vetor de características."],"metadata":{"id":"-MNTsud5xIE4"}},{"cell_type":"code","metadata":{"id":"GWa2vfOzYvzn"},"source":["def findCosineSimilarity(source_representation, test_representation):\n","    a = np.matmul(np.transpose(source_representation), test_representation)\n","    b = np.sum(np.multiply(source_representation, source_representation))\n","    c = np.sum(np.multiply(test_representation, test_representation))\n","    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n","\n","def findEuclideanDistance(source_representation, test_representation):\n","    euclidean_distance = source_representation - test_representation\n","    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n","    euclidean_distance = np.sqrt(euclidean_distance)\n","    return euclidean_distance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0bCJE4IZTGh"},"source":["### A função verifyFace recebe duas imagens e calcula a similaridade entre elas. Se a similaridade for menor que epsilon, afirma-se que as duas imagens são de uma mesma pessa."]},{"cell_type":"code","metadata":{"id":"XuYzucEcY1U8"},"source":["epsilon = 0.0040\n","\n","def verifyFace(img1, img2):\n","\n","    img1_representation = vgg_face_descriptor.predict(img1, steps=None)[0,:]\n","    img2_representation = vgg_face_descriptor.predict(img2, steps=None)[0,:]\n","\n","    cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n","    euclidean_distance = findEuclideanDistance(img1_representation, img2_representation)\n","\n","    print(\"Similaridade com distancia do cosseno: \",cosine_similarity)\n","    print(\"Similaridade com distancia euclideana: \",euclidean_distance)\n","\n","    if(cosine_similarity < epsilon):\n","        print(\"Verificado! Mesma pessoa!\")\n","    else:\n","        print(\"Não-verificado! Não são a mesma pessoa!\")\n","\n","    f = plt.figure()\n","    f.add_subplot(1,2, 1)\n","    plt.imshow(np.squeeze(img1))\n","    plt.xticks([]); plt.yticks([])\n","    f.add_subplot(1,2, 2)\n","    plt.imshow(np.squeeze(img2))\n","    plt.xticks([]); plt.yticks([])\n","    plt.show(block=True)\n","    print(\"-----------------------------------------\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Verificando a similaridade entre imagens (6pt)"],"metadata":{"id":"w9lgb2ayyES4"}},{"cell_type":"markdown","metadata":{"id":"Wk2xKaknZboC"},"source":["Para 4 pares de imagens da base da AT&T e faça uma verificação entre elas, chamando a função verifyFace().\n","\n","Antes de usar o VGG como um extrator de caracteristicas, normalize os dados dividindo os pixels por 255. Além disso, re-escalone as imagesn para o formato 224x224. Use a biblioteca OpenCV (cv2).\n","\n","Faça para os pares : 64 e 33, 3 e 7, 40 e 44, 100 e 200."]},{"cell_type":"code","metadata":{"id":"LHGlWALSY40r"},"source":["import cv2\n","\n","# Ajuste as imagens para a entrada do modelo VGG\n","\n","# exemplo, para o par 64 e 33 :\n","\n","# Todo : Normaliza entre 0 e 1 , dividindo por 255\n","img1 = X[64,:,:,:] # Todo\n","img2 = X[33,:,:,:] # Todo\n","\n","# Redimensione a imagem para (224,224) e coloca a primeira dimensão unitária\n","img1 = cv2.resize( # Todo\n","img2 = cv2.resize( # Todo\n","\n","# lembre-se de expandir a primeira dimensão, pois nosso lote aqui é de 1 imagem\n","img1 = np.expand_dims( # Todo ..\n","img2 = np.expand_dims( # Todo ..\n","\n","verifyFace(img1, img2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmJgumuzdzpu"},"source":["# 4. Transferência de aprendizado (50pt)\n"]},{"cell_type":"markdown","source":["Estude o tutorial do [link](https://www.tensorflow.org/tutorials/images/transfer_learning) e aplique o mesmo procedimento para ajustar um modelo previamente treinado com imagens da imagenet.\n","Use o MobileNetV2 como modelo base.\n","\n","Faça o procedimento em duas etapas:\n","\n","\n","1.   Congele todas as camadas exceto as novas que você adicinou ao modelo. Treine.\n","2.   Libere todas as camadas paar o treinamento e treine novamente com um Learning Rate bem pequeno (um décimo do realizado no ítem 1)."],"metadata":{"id":"HRUSJrfayPyI"}},{"cell_type":"code","metadata":{"id":"zBOd8fDNd5Kz"},"source":["# Usando o mobileNet, as imagens devem ter entrada de 160x160x3 e normalizadas entre 0 e 1.\n","# Use a funçao abaixo para fazer o trabalho, conjuntamente com tf.data.Dataset.from_tensor_slices\n","\n","IMG_SIZE = 160 # All images will be resized to 160x160\n","\n","def format_example(image, label):\n","  image = tf.cast(image, tf.float32)\n","  image = (image/127.5) - 1\n","  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n","  return image, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfvqj3_aetnn"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9evGRp6Jeapk"},"source":["# Tensorflow tem funções especĩficas para carregar os dados. Veja tf.data.Dataset\n","\n","raw_train = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n","raw_test = tf.data.Dataset.from_tensor_slices((X_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKVBuiclecrl"},"source":["train = raw_train.map(format_example)\n","test = raw_test.map(format_example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5a7O4D1eyd0"},"source":["Seus dados devem ter o formato :\n","\n","\n","\n","```\n","TensorShape([Dimension(280), Dimension(160), Dimension(160), Dimension(3)])\n","\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"td9-5NqzdkVI"},"source":["train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n","test_batches = test.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2d83DOcdfDw_"},"source":["## 4.1. Execute os passos (35pt):\n","\n","\n"]},{"cell_type":"markdown","source":["1. Carregue o modelo pré-treinado do MobileNet, remova a última camada.\n","2. Adicione uma camdada de Global Average Pooling 2D (GAP)\n","3. Adicione uma camada densa para ajustar ao seu número de classes e use ativação softmax\n","4. Use função de custo loss='sparse_categorical_crossentropy'\n","5. Dentre os dados de treinamento, reserve 10% para validação do modelo.\n","6. Treine por 10 épocas.\n","7. Plote os gráficos de custo do treino e validação\n"],"metadata":{"id":"6ZZrM-i7OOVT"}},{"cell_type":"code","source":["# Todo"],"metadata":{"id":"Qb3rfq47yWNf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2. Fazendo testes (13pt)"],"metadata":{"id":"xomY16Xuz_1_"}},{"cell_type":"markdown","metadata":{"id":"p5wzK5IfgWHu"},"source":["Analize os gráficos. Você provavelmente deve ter observado overfitting.\n","Aplique algumas regularizações no modelo, para tentar reduzir o super-ajuste.\n","\n","\n","\n","1.   Dropout, antes da camada densa, de 50%\n","2.   Regularização nos pesos da camada densa (L1 ou L2)\n","3.   Dropout antes da camada de GAP\n","\n","\n","\n","Veja exemplos no [link](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)"]},{"cell_type":"markdown","metadata":{"id":"e5pM1K_Jg_pk"},"source":["### Responda (2pt)\n","\n","**ToDo** - com qual configuração conseguiu resolver o overfitting?"]}]}