{"cells":[{"cell_type":"markdown","metadata":{"id":"CSR-9BNq3Bu0"},"source":["# Lab 7 - BCC406\n","\n","## REDES NEURAIS E APRENDIZAGEM EM PROFUNDIDADE\n","\n","## Detecção e Segmentação de objetos\n","\n","### Prof. Eduardo e Prof. Pedro\n","\n","Objetivos:\n","\n","- Parte I : Detecção de objetos\n","\n","- Parte II : Segmentação de imagens na [Oxford Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)\n","\n","Data da entrega : 04/12\n","\n","- Este notebook é baseado em tensorflow e Keras.\n","- Execute todo notebook e salve tudo em um PDF **nomeado** como \"NomeSobrenome-LabX.pdf\"\n","- Envie o PDF via google [FORM](https://forms.gle/JrVPvtzCa39ojmrQ6)"]},{"cell_type":"markdown","metadata":{"id":"YvnrJOxWChwo"},"source":["# Parte I - Detecção de Objetos (60pt)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oZavitsE6N24"},"source":["Execute o tutorial do [link](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/tf2_object_detection.ipynb#scrollTo=HtwrSqvakTNn). Faça um teste com os seguintes modelos:\n","\n","- EfficientDet D0 512x512\n","- SSD MobileNet V2 FPNLite 320x320\n","- SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\n","- Faster R-CNN ResNet50 V1 640x640\n","- Mask R-CNN Inception ResNet V2 1024x1024\n","\n","Teste com imagens de:\n","\n","- Praia\n","- Cachorros\n","- Pássartos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"t_ACFq1_zPd_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (1.24.3)\n","Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (3.20.3)\n"]}],"source":["# This Colab requires a recent numpy version.\n","!pip install numpy==1.24.3\n","!pip install protobuf==3.20.3\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4045,"status":"ok","timestamp":1704317235739,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"HF6j_xp2zYZX"},"outputs":[],"source":["import os\n","import pathlib\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import io\n","import scipy.misc\n","import numpy as np\n","from six import BytesIO\n","from PIL import Image, ImageDraw, ImageFont\n","from six.moves.urllib.request import urlopen\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","tf.get_logger().setLevel('ERROR')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704317235739,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"5r17XhlA1oVG"},"outputs":[],"source":["# @title Run this!!\n","\n","def load_image_into_numpy_array(path):\n","  \"\"\"Load an image from file into a numpy array.\n","\n","  Puts image into numpy array to feed into tensorflow graph.\n","  Note that by convention we put it into a numpy array with shape\n","  (height, width, channels), where channels=3 for RGB.\n","\n","  Args:\n","    path: the file path to the image\n","\n","  Returns:\n","    uint8 numpy array with shape (img_height, img_width, 3)\n","  \"\"\"\n","  image = None\n","  if(path.startswith('http')):\n","    response = urlopen(path)\n","    image_data = response.read()\n","    image_data = BytesIO(image_data)\n","    image = Image.open(image_data)\n","  else:\n","    image_data = tf.io.gfile.GFile(path, 'rb').read()\n","    image = Image.open(BytesIO(image_data))\n","\n","  (im_width, im_height) = image.size\n","  return np.array(image.getdata()).reshape(\n","      (1, im_height, im_width, 3)).astype(np.uint8)\n","\n","\n","ALL_MODELS = {\n","'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n","'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n","'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n","'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n","'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n","'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n","'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n","'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n","'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n","'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n","'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n","'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n","'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n","'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n","'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n","'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n","'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n","'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n","'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n","'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n","'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n","'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n","'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n","'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n","'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n","'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n","'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n","'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n","'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n","'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n","'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n","'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n","'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n","'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n","'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n","'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n","'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n","'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n","'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n","}\n","\n","IMAGES_FOR_TEST = {\n","  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n","  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n","  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n","  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n","  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n","  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n","  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n","  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n","}\n","\n","COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n"," (0, 2),\n"," (1, 3),\n"," (2, 4),\n"," (0, 5),\n"," (0, 6),\n"," (5, 7),\n"," (7, 9),\n"," (6, 8),\n"," (8, 10),\n"," (5, 6),\n"," (5, 11),\n"," (6, 12),\n"," (11, 12),\n"," (11, 13),\n"," (13, 15),\n"," (12, 14),\n"," (14, 16)]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7473,"status":"ok","timestamp":1704317243210,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"z7OruktvzkE_","outputId":"932a640f-534e-4a78-8ddd-da89c9bf808e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'models'...\n","remote: Enumerating objects: 4065, done.\u001b[K\n","remote: Counting objects: 100% (4065/4065), done.\u001b[K\n","remote: Compressing objects: 100% (3104/3104), done.\u001b[K\n","remote: Total 4065 (delta 1184), reused 1924 (delta 901), pack-reused 0\u001b[K\n","Receiving objects: 100% (4065/4065), 54.72 MiB | 14.11 MiB/s, done.\n","Resolving deltas: 100% (1184/1184), done.\n","Updating files: 100% (3677/3677), done.\n"]}],"source":["# Clone the tensorflow models repository\n","!git clone --depth 1 https://github.com/tensorflow/models"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56265,"status":"ok","timestamp":1704317299465,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"22gv8ep4zmCf","outputId":"c7ce66ce-3e34-409e-bfd1-e05f9999a99c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Processing /content/models/research\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting avro-python3 (from object-detection==0.1)\n","  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting apache-beam (from object-detection==0.1)\n","  Downloading apache_beam-2.52.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/14.7 MB 36.0 MB/s eta 0:00:00\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (9.4.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (4.9.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.0.7)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (21.6.0)\n","Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.7)\n","Collecting lvis (from object-detection==0.1)\n","  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.11.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.5.3)\n","Collecting tf-models-official\u003e=2.5.1 (from object-detection==0.1)\n","  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 51.3 MB/s eta 0:00:00\n","Collecting tensorflow_io (from object-detection==0.1)\n","  Downloading tensorflow_io-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.3 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.3/47.3 MB 9.5 MB/s eta 0:00:00\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.15.0)\n","Collecting pyparsing==2.4.7 (from object-detection==0.1)\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.8/67.8 kB 4.8 MB/s eta 0:00:00\n","Collecting sacrebleu\u003c=2.2.0 (from object-detection==0.1)\n","  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.6/116.6 kB 15.7 MB/s eta 0:00:00\n","Collecting portalocker (from sacrebleu\u003c=2.2.0-\u003eobject-detection==0.1)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu\u003c=2.2.0-\u003eobject-detection==0.1) (2023.6.3)\n","Requirement already satisfied: tabulate\u003e=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu\u003c=2.2.0-\u003eobject-detection==0.1) (0.9.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu\u003c=2.2.0-\u003eobject-detection==0.1) (1.24.3)\n","Collecting colorama (from sacrebleu\u003c=2.2.0-\u003eobject-detection==0.1)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.5.0)\n","Requirement already satisfied: google-api-python-client\u003e=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.84.0)\n","Collecting immutabledict (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1)\n","  Downloading immutabledict-4.1.0-py3-none-any.whl (4.5 kB)\n","Requirement already satisfied: kaggle\u003e=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.5.16)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (4.1.3)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (4.8.1.78)\n","Requirement already satisfied: psutil\u003e=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (5.9.5)\n","Requirement already satisfied: py-cpuinfo\u003e=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (9.0.0)\n","Requirement already satisfied: pyyaml\u003e=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (6.0.1)\n","Collecting sentencepiece (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 31.2 MB/s eta 0:00:00\n","Collecting seqeval (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1)\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 5.4 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (4.9.4)\n","Requirement already satisfied: tensorflow-hub\u003e=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.15.0)\n","Collecting tensorflow-model-optimization\u003e=0.4.1 (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1)\n","  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.2/241.2 kB 17.2 MB/s eta 0:00:00\n","Collecting tensorflow-text~=2.15.0 (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1)\n","  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 49.1 MB/s eta 0:00:00\n","Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.15.0)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003eobject-detection==0.1) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003eobject-detection==0.1) (2023.3.post1)\n","Requirement already satisfied: absl-py\u003e=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim-\u003eobject-detection==0.1) (1.4.0)\n","Collecting crcmod\u003c2.0,\u003e=1.7 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading crcmod-1.7.tar.gz (89 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.7/89.7 kB 12.0 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting orjson\u003c4,\u003e=3.9.7 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.7/138.7 kB 20.4 MB/s eta 0:00:00\n","Collecting dill\u003c0.3.2,\u003e=0.3.1.1 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.0/152.0 kB 24.7 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (2.2.1)\n","Collecting fastavro\u003c2,\u003e=0.23.6 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading fastavro-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 97.1 MB/s eta 0:00:00\n","Collecting fasteners\u003c1.0,\u003e=0.3 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n","Requirement already satisfied: grpcio!=1.48.0,\u003c2,\u003e=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (1.60.0)\n","Collecting hdfs\u003c3.0.0,\u003e=2.1.0 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading hdfs-2.7.3.tar.gz (43 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.5/43.5 kB 6.9 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: httplib2\u003c0.23.0,\u003e=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (0.22.0)\n","Collecting js2py\u003c1,\u003e=0.74 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 80.9 MB/s eta 0:00:00\n","Requirement already satisfied: jsonschema\u003c5.0.0,\u003e=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (4.19.2)\n","Collecting objsize\u003c0.7.0,\u003e=0.6.1 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n","Requirement already satisfied: packaging\u003e=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (23.2)\n","Collecting pymongo\u003c5.0.0,\u003e=3.8.0 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 677.1/677.1 kB 67.1 MB/s eta 0:00:00\n","Requirement already satisfied: proto-plus\u003c2,\u003e=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (1.23.0)\n","Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,\u003c4.26.0,\u003e=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (3.20.3)\n","Requirement already satisfied: pydot\u003c2,\u003e=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (1.4.2)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (2.31.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (4.5.0)\n","Collecting zstandard\u003c1,\u003e=0.18.0 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 118.5 MB/s eta 0:00:00\n","Requirement already satisfied: pyarrow\u003c12.0.0,\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam-\u003eobject-detection==0.1) (10.0.1)\n","Collecting pyarrow-hotfix\u003c1 (from apache-beam-\u003eobject-detection==0.1)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Requirement already satisfied: cycler\u003e=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis-\u003eobject-detection==0.1) (0.12.1)\n","Requirement already satisfied: kiwisolver\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis-\u003eobject-detection==0.1) (1.4.5)\n","Requirement already satisfied: opencv-python\u003e=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis-\u003eobject-detection==0.1) (4.8.0.76)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eobject-detection==0.1) (1.2.0)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eobject-detection==0.1) (4.47.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem==0.35.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io-\u003eobject-detection==0.1) (0.35.0)\n","Requirement already satisfied: google-auth\u003c3.0.0dev,\u003e=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client\u003e=1.6.7-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.17.3)\n","Requirement already satisfied: google-auth-httplib2\u003e=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client\u003e=1.6.7-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.1.1)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,\u003c3.0.0dev,\u003e=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client\u003e=1.6.7-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.11.1)\n","Requirement already satisfied: uritemplate\u003c5,\u003e=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client\u003e=1.6.7-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (4.1.1)\n","Collecting docopt (from hdfs\u003c3.0.0,\u003e=2.1.0-\u003eapache-beam-\u003eobject-detection==0.1)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: tzlocal\u003e=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py\u003c1,\u003e=0.74-\u003eapache-beam-\u003eobject-detection==0.1) (5.2)\n","Collecting pyjsparser\u003e=2.5.1 (from js2py\u003c1,\u003e=0.74-\u003eapache-beam-\u003eobject-detection==0.1)\n","  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: attrs\u003e=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003c5.0.0,\u003e=4.0.0-\u003eapache-beam-\u003eobject-detection==0.1) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications\u003e=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003c5.0.0,\u003e=4.0.0-\u003eapache-beam-\u003eobject-detection==0.1) (2023.11.2)\n","Requirement already satisfied: referencing\u003e=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003c5.0.0,\u003e=4.0.0-\u003eapache-beam-\u003eobject-detection==0.1) (0.32.0)\n","Requirement already satisfied: rpds-py\u003e=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003c5.0.0,\u003e=4.0.0-\u003eapache-beam-\u003eobject-detection==0.1) (0.15.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle\u003e=1.3.9-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2023.11.17)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle\u003e=1.3.9-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (4.66.1)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle\u003e=1.3.9-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (8.0.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle\u003e=1.3.9-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle\u003e=1.3.9-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (6.1.0)\n","Collecting dnspython\u003c3.0.0,\u003e=1.16.0 (from pymongo\u003c5.0.0,\u003e=3.8.0-\u003eapache-beam-\u003eobject-detection==0.1)\n","  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.4/300.4 kB 34.5 MB/s eta 0:00:00\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.24.0-\u003eapache-beam-\u003eobject-detection==0.1) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.24.0-\u003eapache-beam-\u003eobject-detection==0.1) (3.6)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.6.3)\n","Requirement already satisfied: flatbuffers\u003e=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.5.4)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.2.0)\n","Requirement already satisfied: h5py\u003e=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (3.9.0)\n","Requirement already satisfied: libclang\u003e=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.2.0)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (67.7.2)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.4.0)\n","Requirement already satisfied: wrapt\u003c1.15,\u003e=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.14.1)\n","Requirement already satisfied: tensorboard\u003c2.16,\u003e=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.15.1)\n","Requirement already satisfied: tensorflow-estimator\u003c2.16,\u003e=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.15.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization\u003e=0.4.1-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.1.8)\n","Requirement already satisfied: pyasn1\u003e=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.5.1)\n","Requirement already satisfied: pyasn1-modules\u003e=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.3.0)\n","Requirement already satisfied: rsa\u003e=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (4.9)\n","Requirement already satisfied: scikit-learn\u003e=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.2.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (8.1.7)\n","Requirement already satisfied: etils[enp,epath,etree]\u003e=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.6.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.14.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.10.2)\n","Requirement already satisfied: array-record\u003e=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.5.0)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.42.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]\u003e=0.9.0-\u003etensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]\u003e=0.9.0-\u003etensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (6.1.1)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]\u003e=0.9.0-\u003etensorflow-datasets-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (3.17.0)\n","Requirement already satisfied: googleapis-common-protos\u003c2.0.dev0,\u003e=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,\u003c3.0.0dev,\u003e=1.31.5-\u003egoogle-api-python-client\u003e=1.6.7-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.62.0)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3.0.0dev,\u003e=1.19.0-\u003egoogle-api-python-client\u003e=1.6.7-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (5.3.2)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.3.2)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (3.2.0)\n","Requirement already satisfied: google-auth-oauthlib\u003c2,\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.2.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (3.5.1)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.7.2)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (3.0.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-\u003ekaggle\u003e=1.3.9-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (0.5.1)\n","Requirement already satisfied: text-unidecode\u003e=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify-\u003ekaggle\u003e=1.3.9-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.3)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib\u003c2,\u003e=0.5-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (1.3.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (2.1.3)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c2,\u003e=0.5-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow~=2.15.0-\u003etf-models-official\u003e=2.5.1-\u003eobject-detection==0.1) (3.2.2)\n","Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, hdfs, seqeval, pyjsparser, docopt\n","  Building wheel for object-detection (setup.py): started\n","  Building wheel for object-detection (setup.py): finished with status 'done'\n","  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1697356 sha256=e1ba224cfbccc404f4fabf9418808f50ef8bc30feaa28ff760e452f313a2541b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9ehfbhy6/wheels/53/dd/70/2de274d6c443c69d367bd6a5606f95e5a6df61aacf1435ec0d\n","  Building wheel for avro-python3 (setup.py): started\n","  Building wheel for avro-python3 (setup.py): finished with status 'done'\n","  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43992 sha256=ac5da198f15af463ffd104e4b7078bf91c5d904a66cb3894ace461239713bbe9\n","  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n","  Building wheel for crcmod (setup.py): started\n","  Building wheel for crcmod (setup.py): finished with status 'done'\n","  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31408 sha256=f988f21c197a9748b2d11322678c3273cd0528c6642659941a406c70a2e7a561\n","  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n","  Building wheel for dill (setup.py): started\n","  Building wheel for dill (setup.py): finished with status 'done'\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78540 sha256=aea90af7eae51e86348609623c309bf9dc61494677d336022f51eecf19c587bd\n","  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n","  Building wheel for hdfs (setup.py): started\n","  Building wheel for hdfs (setup.py): finished with status 'done'\n","  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=8bf165f7bfad9e412cd191b7ae5379b450e26be95237787f3db210b5c31d9f7e\n","  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n","  Building wheel for seqeval (setup.py): started\n","  Building wheel for seqeval (setup.py): finished with status 'done'\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=0abe3fb9c325c7bf31501930aaed3ab42a1ce8f2d3b46083e461ce2a9eb0bff4\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","  Building wheel for pyjsparser (setup.py): started\n","  Building wheel for pyjsparser (setup.py): finished with status 'done'\n","  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=06fcd17018a203e55e3885b572b506b9fe0a42e9d371b3213c1b6da0e34bb905\n","  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n","  Building wheel for docopt (setup.py): started\n","  Building wheel for docopt (setup.py): finished with status 'done'\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=acc36baff3c0235b27dfb4246a4645a362fcf0ad2835ae17d43c0205e08d9599\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built object-detection avro-python3 crcmod dill hdfs seqeval pyjsparser docopt\n","Installing collected packages: sentencepiece, pyjsparser, docopt, crcmod, zstandard, tensorflow-model-optimization, tensorflow_io, pyparsing, pyarrow-hotfix, portalocker, orjson, objsize, js2py, immutabledict, fasteners, fastavro, dnspython, dill, colorama, avro-python3, sacrebleu, pymongo, hdfs, seqeval, lvis, apache-beam, tensorflow-text, tf-models-official, object-detection\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.1.1\n","    Uninstalling pyparsing-3.1.1:\n","      Successfully uninstalled pyparsing-3.1.1\n","Successfully installed apache-beam-2.52.0 avro-python3-1.10.2 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.4.2 docopt-0.6.2 fastavro-1.9.2 fasteners-0.19 hdfs-2.7.3 immutabledict-4.1.0 js2py-0.74 lvis-0.5.3 object-detection-0.1 objsize-0.6.1 orjson-3.9.10 portalocker-2.8.2 pyarrow-hotfix-0.6 pyjsparser-2.7.1 pymongo-4.6.1 pyparsing-2.4.7 sacrebleu-2.2.0 sentencepiece-0.1.99 seqeval-1.2.2 tensorflow-model-optimization-0.7.5 tensorflow-text-2.15.0 tensorflow_io-0.35.0 tf-models-official-2.15.0 zstandard-0.22.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n"]}],"source":["%%bash\n","sudo apt install -y protobuf-compiler\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cp object_detection/packages/tf2/setup.py .\n","python -m pip install .\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":499,"status":"error","timestamp":1704317315145,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"_EmDS-HJznrO","outputId":"a43c3279-0bac-4728-8ae0-6918f5551bf6"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-9-c5b38f3cb152\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualization_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mviz_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/object_detection/utils/ops.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtf_slim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstandard_fields\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshape_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_slim/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_slim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_slim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 45\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtf_slim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_slim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_slim/losses/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_slim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetric_learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_slim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_slim/losses/metric_learning.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 39\u001b[0;31m   \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sklearn.metrics'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0mHAS_SKLEARN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadpool_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from .validation import (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m    606\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--\u003e 608\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_private\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_assert_valid_refcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gen_alignment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorators\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 480\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_nep50_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdesired\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \"\"\"\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# here to save on the order of 10 ms of import time for most users\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 311\u001b[0;31m         \u001b[0;31m# The previous way Tester was imported also had a side effect of adding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;31m# the full `numpy.testing` namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'testing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute '_no_nep50_warning'"]}],"source":["from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as viz_utils\n","from object_detection.utils import ops as utils_ops\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300427,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"G56DsWZJzqEL"},"outputs":[],"source":["PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n","category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300427,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"btHvNyPlzvfQ"},"outputs":[],"source":["#@title Model Selection { display-mode: \"form\", run: \"auto\" }\n","model_display_name = 'EfficientDet D6 1280x1280' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n","model_handle = ALL_MODELS[model_display_name]\n","\n","print('Selected model:'+ model_display_name)\n","print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300427,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"Ola1wkrIzzR4"},"outputs":[],"source":["print('loading model...')\n","hub_model = hub.load(model_handle)\n","print('model loaded!')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300427,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"V7UWjyKfz1A9"},"outputs":[],"source":["#@title Image Selection (don't forget to execute the cell!) { display-mode: \"form\"}\n","selected_image = 'Dogs' # @param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n","flip_image_horizontally = False #@param {type:\"boolean\"}\n","convert_image_to_grayscale = False #@param {type:\"boolean\"}\n","\n","image_path = IMAGES_FOR_TEST[selected_image]\n","image_np = load_image_into_numpy_array(image_path)\n","\n","# Flip horizontally\n","if(flip_image_horizontally):\n","  image_np[0] = np.fliplr(image_np[0]).copy()\n","\n","# Convert image to grayscale\n","if(convert_image_to_grayscale):\n","  image_np[0] = np.tile(\n","    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n","\n","plt.figure(figsize=(24,32))\n","plt.imshow(image_np[0])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"XDTwwI5ez2_r"},"outputs":[],"source":["# running inference\n","results = hub_model(image_np)\n","\n","# different object detection models have additional results\n","# all of them are explained in the documentation\n","result = {key:value.numpy() for key,value in results.items()}\n","print(result.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"0INgtQ0vz4yt"},"outputs":[],"source":["label_id_offset = 0\n","image_np_with_detections = image_np.copy()\n","\n","# Use keypoints if available in detections\n","keypoints, keypoint_scores = None, None\n","if 'detection_keypoints' in result:\n","  keypoints = result['detection_keypoints'][0]\n","  keypoint_scores = result['detection_keypoint_scores'][0]\n","\n","viz_utils.visualize_boxes_and_labels_on_image_array(\n","      image_np_with_detections[0],\n","      result['detection_boxes'][0],\n","      (result['detection_classes'][0] + label_id_offset).astype(int),\n","      result['detection_scores'][0],\n","      category_index,\n","      use_normalized_coordinates=True,\n","      max_boxes_to_draw=200,\n","      min_score_thresh=.30,\n","      agnostic_mode=False,\n","      keypoints=keypoints,\n","      keypoint_scores=keypoint_scores,\n","      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n","\n","plt.figure(figsize=(24,32))\n","plt.imshow(image_np_with_detections[0])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"rUa08ki5z8ki"},"outputs":[],"source":["# Handle models with masks:\n","image_np_with_mask = image_np.copy()\n","\n","if 'detection_masks' in result:\n","  # we need to convert np.arrays to tensors\n","  detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n","  detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n","\n","  # Reframe the bbox mask to the image size.\n","  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","            detection_masks, detection_boxes,\n","              image_np.shape[1], image_np.shape[2])\n","  detection_masks_reframed = tf.cast(detection_masks_reframed \u003e 0.5,\n","                                      tf.uint8)\n","  result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n","\n","viz_utils.visualize_boxes_and_labels_on_image_array(\n","      image_np_with_mask[0],\n","      result['detection_boxes'][0],\n","      (result['detection_classes'][0] + label_id_offset).astype(int),\n","      result['detection_scores'][0],\n","      category_index,\n","      use_normalized_coordinates=True,\n","      max_boxes_to_draw=200,\n","      min_score_thresh=.30,\n","      agnostic_mode=False,\n","      instance_masks=result.get('detection_masks_reframed', None),\n","      line_thickness=8)\n","\n","plt.figure(figsize=(24,32))\n","plt.imshow(image_np_with_mask[0])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yW6GkpqQ6Ve5"},"source":["## ToDo : Custo computacional (30pt)"]},{"cell_type":"markdown","metadata":{"id":"vfBBQ-k17D76"},"source":["Compute o custo computacional (tempo de inferência) de cada modelo acima\n","\n","Dica : Use o método \"default_timer\" da biblioteca \"timeit\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"rWHnX5a5yrIi"},"outputs":[],"source":["import timeit\n","import absl.logging\n","absl.logging.set_verbosity(absl.logging.ERROR)\n","\n","# modelos = ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n","modelos = ['CenterNet HourGlass104 512x512']\n","\n","for modelo in modelos:\n","  print(f'Running {modelo}')\n","\n","  model_handle = ALL_MODELS[modelo]\n","  hub_model = hub.load(model_handle)\n","\n","  inicio = timeit.default_timer()\n","  results = hub_model(image_np)\n","  fim = timeit.default_timer()\n","  print ('duracao: %f' % (fim - inicio))"]},{"cell_type":"markdown","metadata":{"id":"XdksL2TFT23R"},"source":["## ToDo : YoloV3 (30pt)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wkSe9QYd7JcW"},"source":["Carregue o YoloV3 pré-treinado (ver [link](https://github.com/christienatashiaarchie/YOLOv3-Object-Detection/blob/master/Yolov3.ipynb)) e execute a inferência nas mesmas imagens testadas com os modelos acima. Calule o custo computacional e compare contra os modelos acima. Qual a sua conclusão? Justifique."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"JNep0xj76zCO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"tnw0mBCB60Ag"},"outputs":[],"source":["import os\n","import scipy.io\n","import scipy.misc\n","import numpy as np\n","import pandas as pd\n","import PIL\n","import struct\n","import cv2\n","from numpy import expand_dims\n","import tensorflow as tf\n","from skimage.transform import resize\n","from keras import backend as K\n","from keras.layers import Input, Lambda, Conv2D, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n","from keras.models import load_model, Model\n","from keras.layers import add, concatenate\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from matplotlib import pyplot\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imshow\n","from matplotlib.patches import Rectangle\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"2TIKcv8X61bq"},"outputs":[],"source":["class WeightReader:\n","\tdef __init__(self, weight_file):\n","\t\twith open(weight_file, 'rb') as w_f:\n","\t\t\tmajor,\t= struct.unpack('i', w_f.read(4))\n","\t\t\tminor,\t= struct.unpack('i', w_f.read(4))\n","\t\t\trevision, = struct.unpack('i', w_f.read(4))\n","\t\t\tif (major*10 + minor) \u003e= 2 and major \u003c 1000 and minor \u003c 1000:\n","\t\t\t\tw_f.read(8)\n","\t\t\telse:\n","\t\t\t\tw_f.read(4)\n","\t\t\ttranspose = (major \u003e 1000) or (minor \u003e 1000)\n","\t\t\tbinary = w_f.read()\n","\t\tself.offset = 0\n","\t\tself.all_weights = np.frombuffer(binary, dtype='float32')\n","\n","\tdef read_bytes(self, size):\n","\t\tself.offset = self.offset + size\n","\t\treturn self.all_weights[self.offset-size:self.offset]\n","\n","\tdef load_weights(self, model):\n","\t\tfor i in range(106):\n","\t\t\ttry:\n","\t\t\t\tconv_layer = model.get_layer('conv_' + str(i))\n","\t\t\t\tprint(\"loading weights of convolution #\" + str(i))\n","\t\t\t\tif i not in [81, 93, 105]:\n","\t\t\t\t\tnorm_layer = model.get_layer('bnorm_' + str(i))\n","\t\t\t\t\tsize = np.prod(norm_layer.get_weights()[0].shape)\n","\t\t\t\t\tbeta  = self.read_bytes(size) # bias\n","\t\t\t\t\tgamma = self.read_bytes(size) # scale\n","\t\t\t\t\tmean  = self.read_bytes(size) # mean\n","\t\t\t\t\tvar   = self.read_bytes(size) # variance\n","\t\t\t\t\tweights = norm_layer.set_weights([gamma, beta, mean, var])\n","\t\t\t\tif len(conv_layer.get_weights()) \u003e 1:\n","\t\t\t\t\tbias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n","\t\t\t\t\tkernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n","\t\t\t\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n","\t\t\t\t\tkernel = kernel.transpose([2,3,1,0])\n","\t\t\t\t\tconv_layer.set_weights([kernel, bias])\n","\t\t\t\telse:\n","\t\t\t\t\tkernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n","\t\t\t\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n","\t\t\t\t\tkernel = kernel.transpose([2,3,1,0])\n","\t\t\t\t\tconv_layer.set_weights([kernel])\n","\t\t\texcept ValueError:\n","\t\t\t\tprint(\"no convolution #\" + str(i))\n","\n","\tdef reset(self):\n","\t\tself.offset = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"sTvnwL92623g"},"outputs":[],"source":["def _conv_block(inp, convs, skip=True):\n","\tx = inp\n","\tcount = 0\n","\tfor conv in convs:\n","\t\tif count == (len(convs) - 2) and skip:\n","\t\t\tskip_connection = x\n","\t\tcount += 1\n","\t\tif conv['stride'] \u003e 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n","\t\tx = Conv2D(conv['filter'],\n","\t\t\t\t   conv['kernel'],\n","\t\t\t\t   strides=conv['stride'],\n","\t\t\t\t   padding='valid' if conv['stride'] \u003e 1 else 'same', # peculiar padding as darknet prefer left and top\n","\t\t\t\t   name='conv_' + str(conv['layer_idx']),\n","\t\t\t\t   use_bias=False if conv['bnorm'] else True)(x)\n","\t\tif conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n","\t\tif conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n","\treturn add([skip_connection, x]) if skip else x\n","\n","def make_yolov3_model():\n","\tinput_image = Input(shape=(None, None, 3))\n","\t# Layer  0 =\u003e 4\n","\tx = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n","\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n","\t\t\t\t\t\t\t\t  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n","\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n","\t# Layer  5 =\u003e 8\n","\tx = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n","\t\t\t\t\t\t{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n","\t# Layer  9 =\u003e 11\n","\tx = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n","\t# Layer 12 =\u003e 15\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n","\t# Layer 16 =\u003e 36\n","\tfor i in range(7):\n","\t\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n","\t\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n","\tskip_36 = x\n","\t# Layer 37 =\u003e 40\n","\tx = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n","\t# Layer 41 =\u003e 61\n","\tfor i in range(7):\n","\t\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n","\t\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n","\tskip_61 = x\n","\t# Layer 62 =\u003e 65\n","\tx = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n","\t# Layer 66 =\u003e 74\n","\tfor i in range(3):\n","\t\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n","\t\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n","\t# Layer 75 =\u003e 79\n","\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n","\t# Layer 80 =\u003e 82\n","\tyolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n","\t\t\t\t\t\t\t  {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n","\t# Layer 83 =\u003e 86\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n","\tx = UpSampling2D(2)(x)\n","\tx = concatenate([x, skip_61])\n","\t# Layer 87 =\u003e 91\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n","\t# Layer 92 =\u003e 94\n","\tyolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n","\t\t\t\t\t\t\t  {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n","\t# Layer 95 =\u003e 98\n","\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n","\tx = UpSampling2D(2)(x)\n","\tx = concatenate([x, skip_36])\n","\t# Layer 99 =\u003e 106\n","\tyolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n","\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n","\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n","\t\t\t\t\t\t\t   {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n","\tmodel = Model(input_image, [yolo_82, yolo_94, yolo_106])\n","\treturn model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"9otIl4-k648h"},"outputs":[],"source":["# define the yolo v3 model\n","yolov3 = make_yolov3_model()\n","\n","# load the weights\n","weight_reader = WeightReader('/content/gdrive/MyDrive/Colab Notebooks/Lab7/yolov3.weights')\n","\n","# set the weights\n","weight_reader.load_weights(yolov3)\n","\n","# save the model to file\n","yolov3.save('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300428,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"l0WMqSMVj6ah"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"8wPI_fZa66ig"},"outputs":[],"source":["def load_image_pixels(filename, shape):\n","  # load image to get its shape\n","  image = load_img(filename)\n","  width, height = image.size\n","\n","  # load image with required size\n","  image = load_img(filename, target_size=shape)\n","  image = img_to_array(image)\n","\n","  # grayscale image normalization\n","  image = image.astype('float32')\n","  image /= 255.0\n","\n","  # add a dimension so that we have one sample\n","  image = expand_dims(image, 0)\n","  return image, width, height"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"n9-21Me86_sF"},"outputs":[],"source":["class BoundBox:\n","  def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n","    self.xmin = xmin\n","    self.ymin = ymin\n","    self.xmax = xmax\n","    self.ymax = ymax\n","    self.objness = objness\n","    self.classes = classes\n","    self.label = -1\n","    self.score = -1\n","\n","  def get_label(self):\n","    if self.label == -1:\n","      self.label = np.argmax(self.classes)\n","\n","    return self.label\n","\n","  def get_score(self):\n","    if self.score == -1:\n","      self.score = self.classes[self.get_label()]\n","    return self.get_score\n","\n","def _sigmoid(x):\n","  return 1. /(1. + np.exp(-x))\n","\n","def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n","\tgrid_h, grid_w = netout.shape[:2]\n","\tnb_box = 3\n","\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n","\tnb_class = netout.shape[-1] - 5\n","\tboxes = []\n","\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n","\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n","\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n","\tnetout[..., 5:] *= netout[..., 5:] \u003e obj_thresh\n","\n","\tfor i in range(grid_h*grid_w):\n","\t\trow = i / grid_w\n","\t\tcol = i % grid_w\n","\t\tfor b in range(nb_box):\n","\t\t\t# 4th element is objectness score\n","\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n","\t\t\tif(objectness.all() \u003c= obj_thresh): continue\n","\t\t\t# first 4 elements are x, y, w, and h\n","\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n","\t\t\tx = (col + x) / grid_w # center position, unit: image width\n","\t\t\ty = (row + y) / grid_h # center position, unit: image height\n","\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n","\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n","\t\t\t# last elements are class probabilities\n","\t\t\tclasses = netout[int(row)][col][b][5:]\n","\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n","\t\t\tboxes.append(box)\n","\treturn boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"BTguozI-7BHQ"},"outputs":[],"source":["def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n","\tnew_w, new_h = net_w, net_h\n","\tfor i in range(len(boxes)):\n","\t\tx_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n","\t\ty_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n","\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n","\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n","\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n","\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"jzQtRstk7Ca6"},"outputs":[],"source":["def _interval_overlap(interval_a, interval_b):\n","\tx1, x2 = interval_a\n","\tx3, x4 = interval_b\n","\tif x3 \u003c x1:\n","\t\tif x4 \u003c x1:\n","\t\t\treturn 0\n","\t\telse:\n","\t\t\treturn min(x2,x4) - x1\n","\telse:\n","\t\tif x2 \u003c x3:\n","\t\t\t return 0\n","\t\telse:\n","\t\t\treturn min(x2,x4) - x3\n","\n","def bbox_iou(box1, box2):\n","\tintersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n","\tintersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n","\tintersect = intersect_w * intersect_h\n","\tw1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n","\tw2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n","\tunion = w1*h1 + w2*h2 - intersect\n","\treturn float(intersect) / union\n","\n","def do_nms(boxes, nms_thresh):\n","\tif len(boxes) \u003e 0:\n","\t\tnb_class = len(boxes[0].classes)\n","\telse:\n","\t\treturn\n","\tfor c in range(nb_class):\n","\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n","\t\tfor i in range(len(sorted_indices)):\n","\t\t\tindex_i = sorted_indices[i]\n","\t\t\tif boxes[index_i].classes[c] == 0: continue\n","\t\t\tfor j in range(i+1, len(sorted_indices)):\n","\t\t\t\tindex_j = sorted_indices[j]\n","\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j]) \u003e= nms_thresh:\n","\t\t\t\t\tboxes[index_j].classes[c] = 0\n","\n","# get all of the results above a threshold\n","def get_boxes(boxes, labels, thresh):\n","\tv_boxes, v_labels, v_scores = list(), list(), list()\n","\t# enumerate all boxes\n","\tfor box in boxes:\n","\t\t# enumerate all possible labels\n","\t\tfor i in range(len(labels)):\n","\t\t\t# check if the threshold for this label is high enough\n","\t\t\tif box.classes[i] \u003e thresh:\n","\t\t\t\tv_boxes.append(box)\n","\t\t\t\tv_labels.append(labels[i])\n","\t\t\t\tv_scores.append(box.classes[i]*100)\n","\t\t\t\t# don't break, many labels may trigger for one box\n","\treturn v_boxes, v_labels, v_scores\n","\n","# draw all results\n","def draw_boxes(filename, v_boxes, v_labels, v_scores):\n","\t# load the image\n","\tdata = pyplot.imread(filename)\n","\t# plot the image\n","\tpyplot.imshow(data)\n","\t# get the context for drawing boxes\n","\tax = pyplot.gca()\n","\n","\t# plot each box\n","\tfor i in range(len(v_boxes)):\n","\t\tbox = v_boxes[i]\n","\t\t# get coordinates\n","\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n","\t\t# calculate width and height of the box\n","\t\twidth, height = x2 - x1, y2 - y1\n","\t\t# create the shape\n","\t\t# create the shape\n","\t\trect = Rectangle((x1, y1), width, height, fill=False, color='red', linewidth=2)\n","\t\t# draw the box\n","\t\tax.add_patch(rect)\n","\t\t# draw text and score in the top left corner\n","\t\t# draw text and score in top left corner\n","\t\tlabel = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n","\t\tpyplot.text(x1, y1, label, color='red')\n","\n","\t# show the plot\n","\tpyplot.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"jV9qbadU7DxN"},"outputs":[],"source":["# define the anchors\n","anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n","\n","# define the probability threshold for detected objects\n","class_threshold = 0.6\n","\n","# define the labels\n","labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n","\t\"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n","\t\"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n","\t\"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n","\t\"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n","\t\"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n","\t\"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n","\t\"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n","\t\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n","\t\"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"SrNyfscU7E8k"},"outputs":[],"source":["input_w, input_h = 416, 416\n","\n","\n","selected_image = 'Dogs' # ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n","IMAGES_FOR_TEST = {\n","  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n","  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n","  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n","  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n","  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n","  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n","  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n","  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n","}\n","image_path = IMAGES_FOR_TEST[selected_image]\n","\n","\n","image, image_w, image_h = load_image_pixels(image_path, (input_w, input_h))\n","\n","# make prediction\n","yhat = yolov3.predict(image)\n","# summarize the shape of the list of arrays\n","print([a.shape for a in yhat])\n","\n","boxes = list()\n","for i in range(len(yhat)):\n","  # decode the output of the network\n","  boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n","\n","# correct the sizes of the bounding boxes for the shape of the image\n","correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n","\n","# suppress non-maximal boxes\n","do_nms(boxes, 0.5)\n","\n","# get the details of the detected objects\n","v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n","\n","# summarize what we found\n","for i in range(len(v_boxes)):\n","  print(v_labels[i], v_scores[i])\n","\n","# draw what we found\n","draw_boxes(image_path, v_boxes, v_labels, v_scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"0N2tyLwZ7H1f"},"outputs":[],"source":["print([a.shape for a in yhat])"]},{"cell_type":"markdown","metadata":{"id":"3tgTyySYgx5M"},"source":["## ToDo : Detectando objetos com dados próprios (Opcional / 20 Pontos Extra)\n"]},{"cell_type":"markdown","metadata":{"id":"52KO6z__7M1h"},"source":["Caso você queira usar as técnicas de detecção de objetos em uma base de dados própria, siga o tutorial do [link](https://colab.research.google.com/github/luxonis/depthai-ml-training/blob/master/colab-notebooks/Easy_Object_Detection_With_Custom_Data_Demo_Training.ipynb). Você também pode se basear no no trabalho do [link](https://medium.com/swlh/tensorflow-2-object-detection-api-with-google-colab-b2af171e81cc).\n","\n","Relate sua experiência e anexe aqui os resultados."]},{"cell_type":"markdown","metadata":{"id":"j_QC_HdhGDMt"},"source":["# Part II - Segmentação (40pt)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PnPcZs5q7PwD"},"source":["## ToDo : Rodando um tutorial (15pt)\n"]},{"cell_type":"markdown","metadata":{"id":"m_7Rx4Y_6psY"},"source":["Estude o tutorial do [link](https://www.tensorflow.org/tutorials/images/segmentation). Rode o código e veja o resultado.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"MQmKthrSBCld"},"outputs":[],"source":["!pip install git+https://github.com/tensorflow/examples.git"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"YQX7R4bhZy5h"},"outputs":[],"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"g87--n2AtyO_"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","from tensorflow_examples.models.pix2pix import pix2pix\n","\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"40ITeStwDwZb"},"outputs":[],"source":["dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300429,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"FD60EbcAQqov"},"outputs":[],"source":["def normalize(input_image, input_mask):\n","  input_image = tf.cast(input_image, tf.float32) / 255.0\n","  input_mask -= 1\n","  return input_image, input_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"2NPlCnBXQwb1"},"outputs":[],"source":["@tf.function\n","def load_image_train(datapoint):\n","  input_image = tf.image.resize(datapoint['image'], (128, 128))\n","  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n","\n","  if tf.random.uniform(()) \u003e 0.5:\n","    input_image = tf.image.flip_left_right(input_image)\n","    input_mask = tf.image.flip_left_right(input_mask)\n","\n","  input_image, input_mask = normalize(input_image, input_mask)\n","\n","  return input_image, input_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"Zf0S67hJRp3D"},"outputs":[],"source":["def load_image_test(datapoint):\n","  input_image = tf.image.resize(datapoint['image'], (128, 128))\n","  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n","\n","  input_image, input_mask = normalize(input_image, input_mask)\n","\n","  return input_image, input_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"yHwj2-8SaQli"},"outputs":[],"source":["TRAIN_LENGTH = info.splits['train'].num_examples\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"39fYScNz9lmo"},"outputs":[],"source":["train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","test = dataset['test'].map(load_image_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"DeFwFDN6EVoI"},"outputs":[],"source":["train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n","train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","test_dataset = test.batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"3N2RPAAW9q4W"},"outputs":[],"source":["def display(display_list):\n","  plt.figure(figsize=(15, 15))\n","\n","  title = ['Input Image', 'True Mask', 'Predicted Mask']\n","\n","  for i in range(len(display_list)):\n","    plt.subplot(1, len(display_list), i+1)\n","    plt.title(title[i])\n","    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n","    plt.axis('off')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"a6u_Rblkteqb"},"outputs":[],"source":["for image, mask in train.take(1):\n","  sample_image, sample_mask = image, mask\n","display([sample_image, sample_mask])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"c6iB4iMvMkX9"},"outputs":[],"source":["OUTPUT_CHANNELS = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"liCeLH0ctjq7"},"outputs":[],"source":["base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n","\n","# Use as ativações dessas camadas\n","layer_names = [\n","    'block_1_expand_relu',   # 64x64\n","    'block_3_expand_relu',   # 32x32\n","    'block_6_expand_relu',   # 16x16\n","    'block_13_expand_relu',  # 8x8\n","    'block_16_project',      # 4x4\n","]\n","layers = [base_model.get_layer(name).output for name in layer_names]\n","\n","# Crie o modelo de extração de características\n","down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n","\n","down_stack.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"p0ZbfywEbZpJ"},"outputs":[],"source":["up_stack = [\n","    pix2pix.upsample(512, 3),  # 4x4 -\u003e 8x8\n","    pix2pix.upsample(256, 3),  # 8x8 -\u003e 16x16\n","    pix2pix.upsample(128, 3),  # 16x16 -\u003e 32x32\n","    pix2pix.upsample(64, 3),   # 32x32 -\u003e 64x64\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"45HByxpVtrPF"},"outputs":[],"source":["def unet_model(output_channels):\n","\n","  # Esta é a última camada do modelo\n","  last = tf.keras.layers.Conv2DTranspose(\n","      output_channels, 3, strides=2,\n","      padding='same', activation='softmax')  #64x64 -\u003e 128x128\n","\n","  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n","  x = inputs\n","\n","  # Downsampling através do modelo\n","  skips = down_stack(x)\n","  x = skips[-1]\n","  skips = reversed(skips[:-1])\n","\n","  # Upsampling e estabelecimento das conexões de salto\n","  for up, skip in zip(up_stack, skips):\n","    x = up(x)\n","    concat = tf.keras.layers.Concatenate()\n","    x = concat([x, skip])\n","\n","  x = last(x)\n","\n","  return tf.keras.Model(inputs=inputs, outputs=x)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"6he36HK5uKAc"},"outputs":[],"source":["model = unet_model(OUTPUT_CHANNELS)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1704317300430,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"sw82qF1Gcovr"},"outputs":[],"source":["tf.keras.utils.plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"UwvIKLZPtxV_"},"outputs":[],"source":["def create_mask(pred_mask):\n","  pred_mask = tf.argmax(pred_mask, axis=-1)\n","  pred_mask = pred_mask[..., tf.newaxis]\n","  return pred_mask[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"YLNsrynNtx4d"},"outputs":[],"source":["def show_predictions(dataset=None, num=1):\n","  if dataset:\n","    for image, mask in dataset.take(num):\n","      pred_mask = model.predict(image)\n","      display([image[0], mask[0], create_mask(pred_mask)])\n","  else:\n","    display([sample_image, sample_mask,\n","             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"X_1CC0T4dho3"},"outputs":[],"source":["show_predictions()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"wHrHsqijdmL6"},"outputs":[],"source":["class DisplayCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    clear_output(wait=True)\n","    show_predictions()\n","    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"StKDH_B9t4SD"},"outputs":[],"source":["EPOCHS = 20\n","VAL_SUBSPLITS = 5\n","VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n","\n","model_history = model.fit(train_dataset, epochs=EPOCHS,\n","                          steps_per_epoch=STEPS_PER_EPOCH,\n","                          validation_steps=VALIDATION_STEPS,\n","                          validation_data=test_dataset,\n","                          callbacks=[DisplayCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"P_mu0SAbt40Q"},"outputs":[],"source":["loss = model_history.history['loss']\n","val_loss = model_history.history['val_loss']\n","\n","epochs = range(EPOCHS)\n","\n","plt.figure()\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss Value')\n","plt.ylim([0, 1])\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"ikrzoG24qwf5"},"outputs":[],"source":["show_predictions(test_dataset, 3)"]},{"cell_type":"markdown","metadata":{"id":"_Nwer7ev7lL2"},"source":["## ToDo : Melhorando o modelo (25pt)"]},{"cell_type":"markdown","metadata":{"id":"IJgw-WHf71hN"},"source":["Use das operações que você conhece para construir uma rede melhor:\n","- Dropout\n","- Convolution2D, Dense, (várias funções de ativação como GeLu, LeakyReLu, etc)\n","- Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, etc.\n","- Use outras arquiteturas para o encoder:  inception, Xception, VGG16, EfficientNet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"StT3s75hpLPr"},"outputs":[],"source":["!pip install git+https://github.com/tensorflow/examples.git"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704317300431,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"quFR7AtLpLP6"},"outputs":[],"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104633,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"NY7KtzVOpLP6"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","from tensorflow_examples.models.pix2pix import pix2pix\n","\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104632,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"D0qr8xL2pLP6"},"outputs":[],"source":["dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104632,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"QcYzYB9QpLP6"},"outputs":[],"source":["def normalize(input_image, input_mask):\n","  input_image = tf.cast(input_image, tf.float32) / 255.0\n","  input_mask -= 1\n","  return input_image, input_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104632,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"nrKuPGaypLP7"},"outputs":[],"source":["@tf.function\n","def load_image_train(datapoint):\n","  input_image = tf.image.resize(datapoint['image'], (128, 128))\n","  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n","\n","  if tf.random.uniform(()) \u003e 0.5:\n","    input_image = tf.image.flip_left_right(input_image)\n","    input_mask = tf.image.flip_left_right(input_mask)\n","\n","  input_image, input_mask = normalize(input_image, input_mask)\n","\n","  return input_image, input_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104632,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"nSA9CpYJpLP7"},"outputs":[],"source":["def load_image_test(datapoint):\n","  input_image = tf.image.resize(datapoint['image'], (128, 128))\n","  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n","\n","  input_image, input_mask = normalize(input_image, input_mask)\n","\n","  return input_image, input_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104632,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"13Mbs9nkpLP7"},"outputs":[],"source":["TRAIN_LENGTH = info.splits['train'].num_examples\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104631,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"AY1c4Y-YpLP7"},"outputs":[],"source":["train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","test = dataset['test'].map(load_image_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104631,"status":"aborted","timestamp":1704317300433,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"d7dLgEvdpLP8"},"outputs":[],"source":["train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n","train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","test_dataset = test.batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104632,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"dJf3q80WpLP8"},"outputs":[],"source":["def display(display_list):\n","  plt.figure(figsize=(15, 15))\n","\n","  title = ['Input Image', 'True Mask', 'Predicted Mask']\n","\n","  for i in range(len(display_list)):\n","    plt.subplot(1, len(display_list), i+1)\n","    plt.title(title[i])\n","    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n","    plt.axis('off')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104632,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"kKba6_J1pLP8"},"outputs":[],"source":["for image, mask in train.take(1):\n","  sample_image, sample_mask = image, mask\n","display([sample_image, sample_mask])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104631,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"p2Tml1T2pLP8"},"outputs":[],"source":["OUTPUT_CHANNELS = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104631,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"cZd0yHbvpLP9"},"outputs":[],"source":["# base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n","\n","# # Use as ativações dessas camadas\n","# layer_names = [\n","#     'block_1_expand_relu',   # 64x64\n","#     'block_3_expand_relu',   # 32x32\n","#     'block_6_expand_relu',   # 16x16\n","#     'block_13_expand_relu',  # 8x8\n","#     'block_16_project',      # 4x4\n","# ]\n","# layers = [base_model.get_layer(name).output for name in layer_names]\n","\n","# # Crie o modelo de extração de características\n","# down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n","\n","# down_stack.trainable = False\n","\n","# ---------------------------------------------------------------------------------------------------\n","# Melhorando o Modelo\n","\n","base_model = tf.keras.applications.EfficientNetB0(input_shape=[128, 128, 3], include_top=False)\n","\n","activation = tf.keras.layers.LeakyReLU(alpha=0.3)\n","\n","dropout_rate = 0.05\n","\n","layer_names = [\n","    'block1a_activation',   # 64x64\n","    'block2a_activation',   # 32x32\n","    'block3a_activation',   # 16x16\n","    'block4a_activation',   # 8x8\n","    'top_activation',       # 4x4\n","]\n","layers = [base_model.get_layer(name).output for name in layer_names]\n","\n","layers = [tf.keras.layers.Dropout(dropout_rate)(layer) for layer in layers]\n","\n","down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n","down_stack.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104631,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"zaJfnAd_pLP9"},"outputs":[],"source":["up_stack = [\n","    pix2pix.upsample(512, 3),  # 4x4 -\u003e 8x8\n","    pix2pix.upsample(256, 3),  # 8x8 -\u003e 16x16\n","    pix2pix.upsample(128, 3),  # 16x16 -\u003e 32x32\n","    pix2pix.upsample(64, 3),   # 32x32 -\u003e 64x64\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104631,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"ABkyGGN7pLP9"},"outputs":[],"source":["def unet_model(output_channels):\n","\n","  # Esta é a última camada do modelo\n","  last = tf.keras.layers.Conv2DTranspose(\n","      output_channels, 3, strides=2,\n","      padding='same', activation='softmax')  #64x64 -\u003e 128x128\n","\n","  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n","  x = inputs\n","\n","  # Downsampling através do modelo\n","  skips = down_stack(x)\n","  x = skips[-1]\n","  skips = reversed(skips[:-1])\n","\n","  # Upsampling e estabelecimento das conexões de salto\n","  for up, skip in zip(up_stack, skips):\n","    x = up(x)\n","    concat = tf.keras.layers.Concatenate()\n","    x = concat([x, skip])\n","\n","  x = last(x)\n","\n","  return tf.keras.Model(inputs=inputs, outputs=x)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104630,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"f2GQ9IRApLP9"},"outputs":[],"source":["model = unet_model(OUTPUT_CHANNELS)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104630,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"jWM9EHihpLP-"},"outputs":[],"source":["tf.keras.utils.plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104630,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"pH4Ed6jtpLP-"},"outputs":[],"source":["def create_mask(pred_mask):\n","  pred_mask = tf.argmax(pred_mask, axis=-1)\n","  pred_mask = pred_mask[..., tf.newaxis]\n","  return pred_mask[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104630,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"LLahWt25pLP_"},"outputs":[],"source":["def show_predictions(dataset=None, num=1):\n","  if dataset:\n","    for image, mask in dataset.take(num):\n","      pred_mask = model.predict(image)\n","      display([image[0], mask[0], create_mask(pred_mask)])\n","  else:\n","    display([sample_image, sample_mask,\n","             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104630,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"POPMQ3xdpLP_"},"outputs":[],"source":["show_predictions()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104629,"status":"aborted","timestamp":1704317300434,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"Uu6A2kXQpLP_"},"outputs":[],"source":["class DisplayCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    clear_output(wait=True)\n","    show_predictions()\n","    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104629,"status":"aborted","timestamp":1704317300435,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"dk6cpblBpLQA"},"outputs":[],"source":["EPOCHS = 20\n","VAL_SUBSPLITS = 5\n","VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n","\n","model_history = model.fit(train_dataset, epochs=EPOCHS,\n","                          steps_per_epoch=STEPS_PER_EPOCH,\n","                          validation_steps=VALIDATION_STEPS,\n","                          validation_data=test_dataset,\n","                          callbacks=[DisplayCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104629,"status":"aborted","timestamp":1704317300435,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"gKc6SZwTpLQA"},"outputs":[],"source":["loss = model_history.history['loss']\n","val_loss = model_history.history['val_loss']\n","\n","epochs = range(EPOCHS)\n","\n","plt.figure()\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss Value')\n","plt.ylim([0, 1])\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":104628,"status":"aborted","timestamp":1704317300435,"user":{"displayName":"CAIO SILAS DE ARAUJO AMARO","userId":"07412125942931372787"},"user_tz":180},"id":"9_eQTglEpLQA"},"outputs":[],"source":["show_predictions(test_dataset, 3)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","provenance":[{"file_id":"1V9Nne6k_DW-dZXsfbjsXDglJKy0KTsc0","timestamp":1704314401476},{"file_id":"16x_RcFyq8u2cyEINfmpg_OoZu0BsRkJm","timestamp":1652117871413}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}